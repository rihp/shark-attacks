{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "\n",
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Questions: \n",
    "- how to add the dataset to gitignore?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SHARK ATTACKS, a project by Roberto Henr√≠quez\n",
    "\n",
    "# PART I: Data cleaning and exploration\n",
    "\n",
    "## Step 0 - Basic knowledge\n",
    "To begin the development of this project, it would be good to hold a minimum understanding of `Shark Attacks`.\n",
    "\n",
    "As I did not know much about this topic at the day the project started, I have recurred to the shark-attack wiki: https://en.wikipedia.org/wiki/Shark_attack\n",
    "\n",
    "With this information in mind, below is the process of data exploration, cleaning, and wrangling.\n",
    "\n",
    "\n",
    "## Step 1 - Defining the dataset path, and importing it to begin basic dataset exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To follow along and access the DataSet, download it from KAGGLE using this link\n",
    "# https://www.kaggle.com/teajay/global-shark-attacks\n",
    "\n",
    "# Once you have downloaded the DataSet, change the following `dataset` variable to match the \n",
    "# path where you have saved the 'attacks.csv' file.\n",
    "\n",
    "dataset = 'attacks.csv' \n",
    "df = pd.read_csv(dataset, encoding='latin-1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will check some basic information about the dataset, in order to formulate a more educated hypothesis which we could actually put to test with the data available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25723, 24)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6312, 24)\n"
     ]
    }
   ],
   "source": [
    "display(df.shape)# To know the shape of the DF\n",
    "print(df.drop_duplicates().shape) # Shape when eliminating duplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, I notice that the shape of the `df` with no duplicates is very small when compared to the whole `df`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Case Number', 'Date', 'Year', 'Type', 'Country', 'Area', 'Location',\n",
       "       'Activity', 'Name', 'Sex ', 'Age', 'Injury', 'Fatal (Y/N)', 'Time',\n",
       "       'Species ', 'Investigator or Source', 'pdf', 'href formula', 'href',\n",
       "       'Case Number.1', 'Case Number.2', 'original order', 'Unnamed: 22',\n",
       "       'Unnamed: 23'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        25-Jun-2018\n",
       "1        18-Jun-2018\n",
       "2        09-Jun-2018\n",
       "3        08-Jun-2018\n",
       "4        04-Jun-2018\n",
       "            ...     \n",
       "25718            NaN\n",
       "25719            NaN\n",
       "25720            NaN\n",
       "25721            NaN\n",
       "25722            NaN\n",
       "Name: Date, Length: 25723, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# I want to take a look at the time structures\n",
    "df.Date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nodupes = df.drop_duplicates()\n",
    "df_nodupes.Date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, with a df smaller in size, I want to see what info is there on the last couple\n",
    "# of columns which have unexplicit\n",
    "df_nodupes[['Case Number.1', 'Case Number.2', 'original order', 'Unnamed: 22', 'Unnamed: 23']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Too many null values... let's count them and let\n",
    "print(df_nodupes.shape)\n",
    "df_nodupes[['Case Number.1', 'Case Number.2', 'original order', 'Unnamed: 22', 'Unnamed: 23']].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If only 1 value in the 'Unnamed: 22' column, and 2 values in the\n",
    "# 'Unnamed: 22' column, I'll not consider this data for my analysis.\n",
    "df_nodupes = df_nodupes.drop(columns=['Unnamed: 22', 'Unnamed: 23'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we'll look at the columns again\n",
    "df_nodupes.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following columns seemed a little bit rare, so i do a value count to find out what they are about\n",
    "df_nodupes['Case Number.1'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nodupes['Case Number.2'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nodupes['original order'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Since 'original order seems like arbitrary indexes, i'll drop it\n",
    "df_nodupes = df_nodupes.drop(columns='original order')\n",
    "df_nodupes.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I want to check out some of the pdf and href\n",
    "print(df_nodupes[['pdf', 'href']].isnull().sum())\n",
    "df_nodupes[['pdf', 'href']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I want to check what is on those pdfs\n",
    "# A random sample of the column \n",
    "\n",
    "# With a FOR loop\n",
    "for i in range(10):\n",
    "    e = random.choice(range(1000))\n",
    "    print(f\"index: {e}, link: {df_nodupes.iloc[e]['href']}\")\n",
    "\n",
    "# With Random sample\n",
    "display(random.sample(list(df_nodupes['href']), 10))\n",
    "\n",
    "# Below are a couple of the links, when I open them, I have found that they are seem quite structured\n",
    "# It could be possible to parse them later down the road and use a REGEX to find more data\n",
    "\n",
    "# I also have ran this column a few times to notice that all pdfs have actually been uploaded to\n",
    "# the same website and have the same naming structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# After some random sampling, I've noticed that the following indexes have some mistakes.. \n",
    "# a REGEX can be used to fix problems like these \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_nodupes.iloc[332]['href'])\n",
    "print(df_nodupes.iloc[324]['href'])\n",
    "print(df_nodupes.iloc[588]['href'])\n",
    "print(df_nodupes.iloc[569]['href'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## It looks still like some of these pdfs are duplicates, even after dropping duplicates :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many times each pdf on the dataframe\n",
    "df_pdf = df_nodupes[\"pdf\"]\n",
    "df_pdf.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop dupes and compare lengths\n",
    "df_pdf_nodupes = df_pdf.drop_duplicates()\n",
    "\n",
    "len(df_pdf) - len(df_pdf_nodupes), 'duped values'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Since the lengths are not the same, I will check if those duplicated entries are only in this column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# there are 20 duplicated values on the pdf columns\n",
    "df_nodupes.duplicated('pdf').value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# But only 18 dupes if we take Location into count\n",
    "df_nodupes.duplicated(['pdf','Location']).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I'll look at the rest of the data now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_nodupes.shape)\n",
    "df_nodupes.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nodupes = df_nodupes.drop_duplicates()\n",
    "df_nodupes.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_nodupes.shape)\n",
    "df_nodupes[[\"Date\", \"Location\", \"pdf\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nodupes.Country.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# While checking the columns 'Species ' and 'Sex ' have unnecesary spaces at the end of the string\n",
    "# to remove these, and also take out the '(Y/N)' from the column 'Fatal'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_label = df_nodupes\n",
    "df_label.columns = ['Case Number', 'Date', 'Year', 'Type', 'Country', 'Area', 'Location',\n",
    "       'Activity', 'Name', 'Sex', 'Age', 'Injury', 'Fatal', 'Time',\n",
    "       'Species', 'Investigator or Source', 'pdf', 'href formula', 'href',\n",
    "       'Case Number.1', 'Case Number.2']\n",
    "df_label.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_label['Fatal'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "remove_spaces = lambda x:  x.remove(' ') if ' ' in x else x\n",
    "\"\"\"\n",
    "\n",
    "df_label['Fatal'] = list(\n",
    "                            map(remove_spaces(\n",
    "                            df_label['Fatal']),\n",
    "                            ))\n",
    "\"\"\"\n",
    "    \n",
    "df_label['Fatal'].value_counts()\n",
    "\n",
    "\n",
    "# I want to see the indexes which have a duplicated pdf row\n",
    "\"\"\"\n",
    "\n",
    "dupes = []\n",
    "for a,b in list(df_label['pdf'].duplicated().items()):\n",
    "    if b:\n",
    "        dupes.append(a)\n",
    "dupes \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "df_label.loc[dupes]\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dfx = df_nodupes[\"pdf\"].value_counts() if "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transform this to sort the shark species\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(df_label['Species'].value_counts().items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @@ Use this to fill null values: \n",
    "# df_clean[\"drive\"] = df_clean.drive.fillna(\"NoTransmision\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Injuries and types of attack\n",
    "The GSAF categorizes scavenging bites on humans as \"questionable incidents.\"\n",
    "\n",
    "## PROVOKED\n",
    "Provoked attacks occur when a human touches, hooks, nets, or otherwise aggravates the animal. Incidents that occur outside of a shark's natural habitat, such as aquariums and research holding-pens, are considered provoked, as are all incidents involving captured sharks. Sometimes humans inadvertently provoke an attack, such as when a surfer accidentally hits a shark with a surf board.\n",
    "\n",
    "## UNPROVOKED\n",
    "- Hit-and-run attack\n",
    "- Sneak Attack\n",
    "- Bump-and-bite attack \n",
    "\n",
    "For more information on how to differentiate PROVOKED vs UNPROVOKED attacks :\n",
    "https://en.wikipedia.org/wiki/Shark_attack#Types_of_attacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since there is no column that states if the attack was provoked or not,\n",
    "# I want to analyze the injury column to distinguish between the cases that were provoked\n",
    "# and those that were unprovoked.\n",
    "\n",
    "random.sample(list(df_label.Injury.value_counts().items()),20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorizing  Provoked and  Unprovoked attacks\n",
    "#df_clean.loc[df_clean[\"trany\"].str.startswith(\"M\"),\"trany\"] = \"Manual\"\n",
    "\n",
    "provoked = ['PROVOKED', 'hook', 'shot']\n",
    "#map(lambda words, x : words in x, provoked, df_nodupes.loc[df_nodupes['Injury'].str])\n",
    "df_nodupes.loc[df_nodupes['Injury'].str]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nodupes.loc[df_nodupes['Injury'].str]\n",
    "\n",
    "# df_provoked = np.where(df_nodupes.Injury.isin(provoked), True, False) \n",
    "\n",
    "# Passing that categorization to a new PROVOKED COLUMN\n",
    "df_nodupes['Provoked'] = df_provoked\n",
    "df_nodupes['Provoked'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df.columns) # To know which are the columns in the DF\n",
    "display(df.count()) # To know how much data are we missin on each column\n",
    "display(df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_label[['Case Number', 'Date', 'Year', 'Type', 'Country', 'Area', 'Location',\n",
    "       'Activity', 'Name', 'Sex', 'Age', 'Injury', 'Fatal', 'Time', 'Species',\n",
    "       'Investigator or Source', 'pdf', 'href',\n",
    "       'Case Number.1', 'Case Number.2']].head(50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
