{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from functools import reduce\n",
    "import random\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic knowledge: \n",
    "To begin this project, it would be good to hold a minimum understanding of the subject we will be analyzing. As I did not know much about this topic at the day the project started, I have recurred to the shark-attack wiki: https://en.wikipedia.org/wiki/Shark_attack\n",
    "\n",
    "\n",
    "\n",
    "# Defining the dataset path, and importing it to begin basic dataset exploration\n",
    "\n",
    "Questions: \n",
    "- how to add the dataset to gitignore?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To follow along and access the DataSet, download it from KAGGLE using this link\n",
    "# https://www.kaggle.com/teajay/global-shark-attacks\n",
    "\n",
    "# Once you have downloaded the DataSet, change the dataset variable to match the \n",
    "# path where you have saved the 'attacks.csv' file.\n",
    "dataset = 'attacks.csv' \n",
    "df = pd.read_csv(dataset, encoding='latin-1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will check some basic information about the dataset, in order to formulate a more educated hypothesis which we could actually put to test with the data available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25723, 24)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6312, 24)\n"
     ]
    }
   ],
   "source": [
    "display(df.shape)# To know the shape of the DF\n",
    "print(df.drop_duplicates().shape) # Shape when eliminating duplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, I notice that the shape of the df with no duplicates is very small when compared to the whole df. This seems weird and since the `drop_duplicates` function ignores time indexes, I'll try to compare both dataframes' time data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Case Number', 'Date', 'Year', 'Type', 'Country', 'Area', 'Location',\n",
       "       'Activity', 'Name', 'Sex ', 'Age', 'Injury', 'Fatal (Y/N)', 'Time',\n",
       "       'Species ', 'Investigator or Source', 'pdf', 'href formula', 'href',\n",
       "       'Case Number.1', 'Case Number.2', 'original order', 'Unnamed: 22',\n",
       "       'Unnamed: 23'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        25-Jun-2018\n",
       "1        18-Jun-2018\n",
       "2        09-Jun-2018\n",
       "3        08-Jun-2018\n",
       "4        04-Jun-2018\n",
       "            ...     \n",
       "25718            NaN\n",
       "25719            NaN\n",
       "25720            NaN\n",
       "25721            NaN\n",
       "25722            NaN\n",
       "Name: Date, Length: 25723, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.Date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        25-Jun-2018\n",
       "1        18-Jun-2018\n",
       "2        09-Jun-2018\n",
       "3        08-Jun-2018\n",
       "4        04-Jun-2018\n",
       "            ...     \n",
       "6307             NaN\n",
       "6308             NaN\n",
       "6309             NaN\n",
       "8702             NaN\n",
       "25722            NaN\n",
       "Name: Date, Length: 6312, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_nodupes = df.drop_duplicates()\n",
    "df_nodupes.Date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Case Number.1</th>\n",
       "      <th>Case Number.2</th>\n",
       "      <th>original order</th>\n",
       "      <th>Unnamed: 22</th>\n",
       "      <th>Unnamed: 23</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018.06.25</td>\n",
       "      <td>2018.06.25</td>\n",
       "      <td>6303.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018.06.18</td>\n",
       "      <td>2018.06.18</td>\n",
       "      <td>6302.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018.06.09</td>\n",
       "      <td>2018.06.09</td>\n",
       "      <td>6301.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018.06.08</td>\n",
       "      <td>2018.06.08</td>\n",
       "      <td>6300.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018.06.04</td>\n",
       "      <td>2018.06.04</td>\n",
       "      <td>6299.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6307</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6309.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6308</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6310.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6309</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8702</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25722</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6312 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Case Number.1 Case Number.2  original order Unnamed: 22 Unnamed: 23\n",
       "0        2018.06.25    2018.06.25          6303.0         NaN         NaN\n",
       "1        2018.06.18    2018.06.18          6302.0         NaN         NaN\n",
       "2        2018.06.09    2018.06.09          6301.0         NaN         NaN\n",
       "3        2018.06.08    2018.06.08          6300.0         NaN         NaN\n",
       "4        2018.06.04    2018.06.04          6299.0         NaN         NaN\n",
       "...             ...           ...             ...         ...         ...\n",
       "6307            NaN           NaN          6309.0         NaN         NaN\n",
       "6308            NaN           NaN          6310.0         NaN         NaN\n",
       "6309            NaN           NaN             NaN         NaN         NaN\n",
       "8702            NaN           NaN             NaN         NaN         NaN\n",
       "25722           NaN           NaN             NaN         NaN         NaN\n",
       "\n",
       "[6312 rows x 5 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now, with a df smaller in size, I want to see what info is there on the last couple\n",
    "# of columns which have unexplicit\n",
    "df_nodupes[['Case Number.1', 'Case Number.2', 'original order', 'Unnamed: 22', 'Unnamed: 23']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6312, 24)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Case Number.1       10\n",
       "Case Number.2       10\n",
       "original order       3\n",
       "Unnamed: 22       6311\n",
       "Unnamed: 23       6310\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Too many null values... let's count them and let\n",
    "print(df_nodupes.shape)\n",
    "df_nodupes[['Case Number.1', 'Case Number.2', 'original order', 'Unnamed: 22', 'Unnamed: 23']].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If only 1 value in the 'Unnamed: 22' column, and 2 values in the\n",
    "# 'Unnamed: 22' column, I'll not consider this data for my analysis.\n",
    "df_nodupes = df_nodupes.drop(columns=['Unnamed: 22', 'Unnamed: 23'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Case Number', 'Date', 'Year', 'Type', 'Country', 'Area', 'Location',\n",
       "       'Activity', 'Name', 'Sex ', 'Age', 'Injury', 'Fatal (Y/N)', 'Time',\n",
       "       'Species ', 'Investigator or Source', 'pdf', 'href formula', 'href',\n",
       "       'Case Number.1', 'Case Number.2', 'original order'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now we'll look at the columns again\n",
    "df_nodupes.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1952.08.04      2\n",
       "1913.08.27.R    2\n",
       "1962.06.11.b    2\n",
       "2006.09.02      2\n",
       "1990.05.10      2\n",
       "               ..\n",
       "1860.08.01      1\n",
       "1978.09.16      1\n",
       "1981.10.19.a    1\n",
       "2007.06.30.b    1\n",
       "1965.03.23      1\n",
       "Name: Case Number.1, Length: 6285, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The following columns seemed a little bit rare, so i do a value count to find out what they are about\n",
    "df_nodupes['Case Number.1'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2005.04.06        2\n",
       "1907.10.16.R      2\n",
       "1915.07.06.a.R    2\n",
       "1913.08.27.R      2\n",
       "1983.06.15        2\n",
       "                 ..\n",
       "1919.01.05        1\n",
       "1973.09.09        1\n",
       "1860.08.01        1\n",
       "1978.09.16        1\n",
       "1965.03.23        1\n",
       "Name: Case Number.2, Length: 6286, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_nodupes['Case Number.2'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "569.0     2\n",
       "4603.0    1\n",
       "4899.0    1\n",
       "810.0     1\n",
       "796.0     1\n",
       "         ..\n",
       "3508.0    1\n",
       "3256.0    1\n",
       "3106.0    1\n",
       "3080.0    1\n",
       "6272.0    1\n",
       "Name: original order, Length: 6308, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_nodupes['original order'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Case Number', 'Date', 'Year', 'Type', 'Country', 'Area', 'Location',\n",
       "       'Activity', 'Name', 'Sex ', 'Age', 'Injury', 'Fatal (Y/N)', 'Time',\n",
       "       'Species ', 'Investigator or Source', 'pdf', 'href formula', 'href',\n",
       "       'Case Number.1', 'Case Number.2'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Since 'original order seems like arbitrary indexes, i'll drop it\n",
    "df_nodupes = df_nodupes.drop(columns='original order')\n",
    "df_nodupes.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pdf     10\n",
      "href    10\n",
      "dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pdf</th>\n",
       "      <th>href</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018.06.25-Wolfe.pdf</td>\n",
       "      <td>http://sharkattackfile.net/spreadsheets/pdf_di...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018.06.18-McNeely.pdf</td>\n",
       "      <td>http://sharkattackfile.net/spreadsheets/pdf_di...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018.06.09-Denges.pdf</td>\n",
       "      <td>http://sharkattackfile.net/spreadsheets/pdf_di...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018.06.08-Arrawarra.pdf</td>\n",
       "      <td>http://sharkattackfile.net/spreadsheets/pdf_di...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018.06.04-Ramos.pdf</td>\n",
       "      <td>http://sharkattackfile.net/spreadsheets/pdf_di...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6307</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6308</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6309</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8702</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25722</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6312 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                            pdf  \\\n",
       "0          2018.06.25-Wolfe.pdf   \n",
       "1        2018.06.18-McNeely.pdf   \n",
       "2         2018.06.09-Denges.pdf   \n",
       "3      2018.06.08-Arrawarra.pdf   \n",
       "4          2018.06.04-Ramos.pdf   \n",
       "...                         ...   \n",
       "6307                        NaN   \n",
       "6308                        NaN   \n",
       "6309                        NaN   \n",
       "8702                        NaN   \n",
       "25722                       NaN   \n",
       "\n",
       "                                                    href  \n",
       "0      http://sharkattackfile.net/spreadsheets/pdf_di...  \n",
       "1      http://sharkattackfile.net/spreadsheets/pdf_di...  \n",
       "2      http://sharkattackfile.net/spreadsheets/pdf_di...  \n",
       "3      http://sharkattackfile.net/spreadsheets/pdf_di...  \n",
       "4      http://sharkattackfile.net/spreadsheets/pdf_di...  \n",
       "...                                                  ...  \n",
       "6307                                                 NaN  \n",
       "6308                                                 NaN  \n",
       "6309                                                 NaN  \n",
       "8702                                                 NaN  \n",
       "25722                                                NaN  \n",
       "\n",
       "[6312 rows x 2 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# I want to check out some of the pdf and href\n",
    "print(df_nodupes[['pdf', 'href']].isnull().sum())\n",
    "df_nodupes[['pdf', 'href']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index: 422, link: http://sharkattackfile.net/spreadsheets/pdf_directory/2015.05.24-Wheatro.pdf\n",
      "index: 825, link: http://sharkattackfile.net/spreadsheets/pdf_directory/2012.01.15-Msungubana.pdf\n",
      "index: 321, link: http://sharkattackfile.net/spreadsheets/pdf_directory/2015.12.25-GrandCanary.pdf\n",
      "index: 600, link: http://sharkattackfile.net/spreadsheets/pdf_directory/2013.11.22-Gardiner\n",
      "index: 711, link: http://sharkattackfile.net/spreadsheets/pdf_directory/2013.01.05-Swaffer.pdf\n",
      "index: 442, link: http://sharkattackfile.net/spreadsheets/pdf_directory/2015.03.16-Bora-Bora.pdf\n",
      "index: 113, link: http://sharkattackfile.net/spreadsheets/pdf_directory/2017.07.23.a-Majorca.pdf\n",
      "index: 370, link: http://sharkattackfile.net/spreadsheets/pdf_directory/2015.09.00-Lautiki.pdf\n",
      "index: 220, link: http://sharkattackfile.net/spreadsheets/pdf_directory/2016.09.07-Bagnol.pdf\n",
      "index: 255, link: http://sharkattackfile.net/spreadsheets/pdf_directory/2016.06.27-Sullivans.pdf\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['http://sharkattackfile.net/spreadsheets/pdf_directory/2018.04.15.b-Longrass.pdf',\n",
       " 'http://sharkattackfile.net/spreadsheets/pdf_directory/2007.06.30.a-Zinner.pdf',\n",
       " 'http://sharkattackfile.net/spreadsheets/pdf_directory/1864.00.00.R-Sumatra.pdf',\n",
       " 'http://sharkattackfile.net/spreadsheets/pdf_directory/1997.08.02.b-Cesar.pdf',\n",
       " 'http://sharkattackfile.net/spreadsheets/pdf_directory/1997.05.31.c-Medvec.pdf',\n",
       " 'http://sharkattackfile.net/spreadsheets/pdf_directory/1932.09.26.R-Fiji.pdf',\n",
       " 'http://sharkattackfile.net/spreadsheets/pdf_directory/1959.05.16-Goldback.pdf',\n",
       " 'http://sharkattackfile.net/spreadsheets/pdf_directory/1850.00.00-LakeNicaragua.pdf',\n",
       " 'http://sharkattackfile.net/spreadsheets/pdf_directory/1960.07.03-Smith.pdf',\n",
       " 'http://sharkattackfile.net/spreadsheets/pdf_directory/2000.09.15-Smith.pdf']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# I want to check what is on those pdfs\n",
    "# A random sample of the column \n",
    "\n",
    "# With a FOR loop\n",
    "for i in range(10):\n",
    "    e = random.choice(range(1000))\n",
    "    print(f\"index: {e}, link: {df_nodupes.iloc[e]['href']}\")\n",
    "\n",
    "# With Random sample\n",
    "display(random.sample(list(df_nodupes['href']), 10))\n",
    "\n",
    "# Below are a couple of the links, when I open them, I have found that they are seem quite structured\n",
    "# It could be possible to parse them later down the road and use a REGEX to find more data\n",
    "\n",
    "# I also have ran this column a few times to notice that all pdfs have actually been uploaded to\n",
    "# the same website and have the same naming structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# After some random sampling, I've noticed that the following indexes have some mistakes.. \n",
    "# a REGEX can be used to fix problems like these \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://sharkattackfile.net/spreadsheets/pdf_directory/http://sharkattackfile.net/spreadsheets/pdf_directory/2015.11.15.a-Engelman.pdf\n",
      "http://sharkattackfile.net/spreadsheets/pdf_directory/http://sharkattackfile.net/spreadsheets/pdf_directory/2015.12.21.a-Brazil.pdf\n",
      "http://sharkattackfile.net/spreadsheets/pdf_directory/http://sharkattackfile.net/spreadsheets/pdf_directory/2014.00.00.b-OceanicWhitetip.pdf\n",
      "http://sharkattackfile.net/spreadsheets/pdf_directory/http://sharkattackfile.net/spreadsheets/pdf_directory/2014.04.03-Armstrong.pdf\n"
     ]
    }
   ],
   "source": [
    "print(df_nodupes.iloc[332]['href'])\n",
    "print(df_nodupes.iloc[324]['href'])\n",
    "print(df_nodupes.iloc[588]['href'])\n",
    "print(df_nodupes.iloc[569]['href'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## It looks still like some of these pdfs are duplicates, even after dropping duplicates :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1929.03.04.a-b.Roads-Aldridge.pdf      2\n",
       "1916.07.12.a-b-Stillwell-Fisher.pdf    2\n",
       "1931.09.21.a-b-Holaday-Barrows.pdf     2\n",
       "1907.10.16.R-HongKong.pdf              2\n",
       "1921.11.27.a-b-Jack.pdf                2\n",
       "                                      ..\n",
       "1983.10.17.a-Kalein.pdf                1\n",
       "1992.01.06-Mauritius.pdf               1\n",
       "1944.10.25.b-Carter.pdf                1\n",
       "2017.06.10.b-Flinders.pdf              1\n",
       "1935.01.24.a-Flat-bottomed-boat.pdf    1\n",
       "Name: pdf, Length: 6291, dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# how many times each pdf on the dataframe\n",
    "df_pdf = df_nodupes[\"pdf\"]\n",
    "df_pdf.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 'duped values')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# drop dupes and compare lengths\n",
    "df_pdf_nodupes = df_pdf.drop_duplicates()\n",
    "\n",
    "len(df_pdf) - len(df_pdf_nodupes), 'duped values'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Since the lengths are not the same, I will check if those duplicated entries are only in this column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    6292\n",
       "True       20\n",
       "dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# there are 20 duplicated values on the pdf columns\n",
    "df_nodupes.duplicated('pdf').value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    6294\n",
       "True       18\n",
       "dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# But only 18 dupes if we take Location into count\n",
    "df_nodupes.duplicated(['pdf','Location']).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I'll look at the rest of the data now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6312, 21)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df_nodupes.shape)\n",
    "df_nodupes.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_nodupes = df_nodupes.drop_duplicates()\n",
    "df_nodupes.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6305, 21)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Location</th>\n",
       "      <th>pdf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>25-Jun-2018</td>\n",
       "      <td>Oceanside, San Diego County</td>\n",
       "      <td>2018.06.25-Wolfe.pdf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>18-Jun-2018</td>\n",
       "      <td>St. Simon Island, Glynn County</td>\n",
       "      <td>2018.06.18-McNeely.pdf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>09-Jun-2018</td>\n",
       "      <td>Habush, Oahu</td>\n",
       "      <td>2018.06.09-Denges.pdf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>08-Jun-2018</td>\n",
       "      <td>Arrawarra Headland</td>\n",
       "      <td>2018.06.08-Arrawarra.pdf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>04-Jun-2018</td>\n",
       "      <td>La Ticla</td>\n",
       "      <td>2018.06.04-Ramos.pdf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6300</th>\n",
       "      <td>1883-1889</td>\n",
       "      <td>Panama Bay 8ºN, 79ºW</td>\n",
       "      <td>ND-0002-JulesPatterson.pdf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6301</th>\n",
       "      <td>1845-1853</td>\n",
       "      <td>Below the English fort, Trincomalee</td>\n",
       "      <td>ND-0001-Ceylon.pdf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6302</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8702</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25722</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6305 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              Date                             Location  \\\n",
       "0      25-Jun-2018          Oceanside, San Diego County   \n",
       "1      18-Jun-2018       St. Simon Island, Glynn County   \n",
       "2      09-Jun-2018                         Habush, Oahu   \n",
       "3      08-Jun-2018                   Arrawarra Headland   \n",
       "4      04-Jun-2018                             La Ticla   \n",
       "...            ...                                  ...   \n",
       "6300     1883-1889                 Panama Bay 8ºN, 79ºW   \n",
       "6301     1845-1853  Below the English fort, Trincomalee   \n",
       "6302           NaN                                  NaN   \n",
       "8702           NaN                                  NaN   \n",
       "25722          NaN                                  NaN   \n",
       "\n",
       "                              pdf  \n",
       "0            2018.06.25-Wolfe.pdf  \n",
       "1          2018.06.18-McNeely.pdf  \n",
       "2           2018.06.09-Denges.pdf  \n",
       "3        2018.06.08-Arrawarra.pdf  \n",
       "4            2018.06.04-Ramos.pdf  \n",
       "...                           ...  \n",
       "6300   ND-0002-JulesPatterson.pdf  \n",
       "6301           ND-0001-Ceylon.pdf  \n",
       "6302                          NaN  \n",
       "8702                          NaN  \n",
       "25722                         NaN  \n",
       "\n",
       "[6305 rows x 3 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df_nodupes.shape)\n",
    "df_nodupes[[\"Date\", \"Location\", \"pdf\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "USA                         2229\n",
       "AUSTRALIA                   1338\n",
       "SOUTH AFRICA                 579\n",
       "PAPUA NEW GUINEA             134\n",
       "NEW ZEALAND                  128\n",
       "                            ... \n",
       "CYPRUS                         1\n",
       "BRITISH ISLES                  1\n",
       "JAVA                           1\n",
       "TASMAN SEA                     1\n",
       "NORTHERN MARIANA ISLANDS       1\n",
       "Name: Country, Length: 212, dtype: int64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_nodupes.Country.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# While checking the columns 'Species ' and 'Sex ' have unnecesary spaces at the end of the string\n",
    "# to remove these, and also take out the '(Y/N)' from the column 'Fatal'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Case Number', 'Date', 'Year', 'Type', 'Country', 'Area', 'Location',\n",
       "       'Activity', 'Name', 'Sex', 'Age', 'Injury', 'Fatal', 'Time', 'Species',\n",
       "       'Investigator or Source', 'pdf', 'href formula', 'href',\n",
       "       'Case Number.1', 'Case Number.2'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_label = df_nodupes\n",
    "df_label.columns = ['Case Number', 'Date', 'Year', 'Type', 'Country', 'Area', 'Location',\n",
    "       'Activity', 'Name', 'Sex', 'Age', 'Injury', 'Fatal', 'Time',\n",
    "       'Species', 'Investigator or Source', 'pdf', 'href formula', 'href',\n",
    "       'Case Number.1', 'Case Number.2']\n",
    "df_label.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "N          4293\n",
       "Y          1388\n",
       "UNKNOWN      71\n",
       " N            7\n",
       "y             1\n",
       "N             1\n",
       "M             1\n",
       "2017          1\n",
       "Name: Fatal, dtype: int64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_label['Fatal'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "map() must have at least two arguments.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m----------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-be4315e8a6ee>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m df_label['Fatal'] = list(\n\u001b[1;32m      4\u001b[0m                             map(remove_spaces(\n\u001b[0;32m----> 5\u001b[0;31m                             df_label['Fatal']),\n\u001b[0m\u001b[1;32m      6\u001b[0m                             ))\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: map() must have at least two arguments."
     ]
    }
   ],
   "source": [
    "remove_spaces = lambda x:  x.remove(' ') if ' ' in x else x\n",
    "\n",
    "df_label['Fatal'] = list(\n",
    "                            map(remove_spaces(\n",
    "                            df_label['Fatal']),\n",
    "                            ))\n",
    "    \n",
    "df_label['Fatal'].value_counts()\n",
    "\n",
    "\n",
    "# I want to see the indexes which have a duplicated pdf row\n",
    "dupes = []\n",
    "for a,b in list(df_label['pdf'].duplicated().items()):\n",
    "    if b:\n",
    "        dupes.append(a)\n",
    "dupes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_label.loc[dupes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfx = df_nodupes[\"pdf\"].value_counts() if "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @@ Use this to fill null values: \n",
    "# df_clean[\"drive\"] = df_clean.drive.fillna(\"NoTransmision\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df.columns) # To know which are the columns in the DF\n",
    "display(df.count()) # To know how much data are we missin on each column\n",
    "display(df.dtypes)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
